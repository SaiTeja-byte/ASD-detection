{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.9524\n",
      "Training Random Forest...\n",
      "Random Forest Accuracy: 1.0000\n",
      "Training SVM...\n",
      "SVM Accuracy: 0.9048\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting Accuracy: 1.0000\n",
      "Training KNN...\n",
      "KNN Accuracy: 0.9524\n",
      "Training XGBoost...\n",
      "XGBoost Accuracy: 1.0000\n",
      "Best Model: Random Forest with Accuracy: 1.0000\n",
      "Best model and scalers have been saved to pickle files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saiha\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:47:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Try to import XGBoost (optional)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_installed = True\n",
    "except ImportError:\n",
    "    xgb_installed = False\n",
    "    print(\"XGBoost not installed. Skipping XGBoost model.\")\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r'Autism_Adolescent_Preprocessed.csv')\n",
    "\n",
    "# Drop missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Extract features and target\n",
    "data_raw = data['Class']\n",
    "features_raw = data[['age', 'gender', 'ethnicity', 'jundice', 'autism', 'relation',\n",
    "                     'contry_of_res', 'A1_Score', 'A2_Score', 'A3_Score', 'A4_Score',\n",
    "                     'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'result']]\n",
    "\n",
    "# Apply both MinMaxScaler and StandardScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "num_features = ['age', 'result']\n",
    "features_transformed = pd.DataFrame(data=features_raw)\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "features_transformed[num_features] = minmax_scaler.fit_transform(features_raw[num_features])\n",
    "\n",
    "# Apply StandardScaler\n",
    "features_transformed[num_features] = standard_scaler.fit_transform(features_transformed[num_features])\n",
    "\n",
    "# One-Hot Encoding\n",
    "features_final = pd.get_dummies(features_transformed)\n",
    "\n",
    "# Encode target variable\n",
    "data_classes = data_raw.apply(lambda x: 1 if x == 'YES' else 0)\n",
    "\n",
    "# Split data\n",
    "np.random.seed(123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, data_classes, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define classification models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=1),\n",
    "    \"SVM\": SVC(kernel='linear', probability=True, random_state=1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=1),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "if xgb_installed:\n",
    "    models[\"XGBoost\"] = XGBClassifier(n_estimators=100, random_state=1, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Train and evaluate models\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "best_model_name = \"\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "# Save the best model\n",
    "if best_model:\n",
    "    with open('best_adolescents_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "# Save the scalers\n",
    "with open('minmax_scaler(adolescents).pkl', 'wb') as f:\n",
    "    pickle.dump(minmax_scaler, f)\n",
    "\n",
    "with open('standard_scaler(adolescents).pkl', 'wb') as f:\n",
    "    pickle.dump(standard_scaler, f)\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}\")\n",
    "print(\"Best model and scalers have been saved to pickle files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model predicts that the individual does not have ASD.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Call the function to get user input and make prediction\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m get_user_input_and_predict()\n",
      "Cell \u001b[1;32mIn[1], line 80\u001b[0m, in \u001b[0;36mget_user_input_and_predict\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_user_input_and_predict\u001b[39m():\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter age (e.g., 12-16): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter gender (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(unique_genders)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124methnicity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter ethnicity (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(unique_ethnicities)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjundice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHad jaundice (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(unique_jundice)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautism\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFamily member with autism (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(unique_austim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontry_of_res\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter country of residence (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(unique_countries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter result (e.g., 10.0): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter relation (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(unique_relations)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA1_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A1_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA2_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A2_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA3_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A3_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA4_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A4_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA5_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A5_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA6_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A6_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA7_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A7_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA8_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A8_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA9_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A9_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA10_Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter A10_Score (0 or 1): \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     90\u001b[0m     }\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# Preprocess the input\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     input_df \u001b[38;5;241m=\u001b[39m preprocess_input(user_input)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained model from the pickle file\n",
    "with open(r'best_adolescents_model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "# Load the scalers used for preprocessing\n",
    "with open(r'minmax_scaler(adolescents).pkl', 'rb') as f:\n",
    "    minmax_scaler = pickle.load(f)\n",
    "\n",
    "with open(r'standard_scaler(adolescents).pkl', 'rb') as f:\n",
    "    standard_scaler = pickle.load(f)\n",
    "\n",
    "# Load the Autism_Adolescent_Data.xlsx file to get the unique values for string labels\n",
    "data = pd.read_csv(r'Autism_Adolescent_Preprocessed.csv')\n",
    "\n",
    "# Get unique values for categorical features\n",
    "unique_genders = data['gender'].unique()\n",
    "unique_ethnicities = data['ethnicity'].unique()\n",
    "unique_jundice = data['jundice'].unique()\n",
    "unique_austim = data['autism'].unique()\n",
    "unique_countries = data['contry_of_res'].unique()\n",
    "unique_relations = data['relation'].unique()\n",
    "\n",
    "# Define the feature columns\n",
    "feature_columns = ['age', 'gender', 'ethnicity', 'jundice', 'autism', 'contry_of_res', 'result',\n",
    "                   'relation', 'A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score',\n",
    "                   'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score']\n",
    "\n",
    "# Preprocess the training data to get the reference columns\n",
    "data.dropna(inplace=True)\n",
    "data_raw = data['Class']\n",
    "features_raw = data[['age', 'gender', 'ethnicity', 'jundice', 'autism', 'relation',\n",
    "                     'contry_of_res', 'A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score',\n",
    "                     'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'result']]\n",
    "\n",
    "features_transform = pd.DataFrame(data=features_raw)\n",
    "\n",
    "# Apply both MinMaxScaler and StandardScaler\n",
    "features_transform[['age', 'result']] = minmax_scaler.transform(features_raw[['age', 'result']])\n",
    "features_transform[['age', 'result']] = standard_scaler.transform(features_transform[['age', 'result']])\n",
    "\n",
    "features_final = pd.get_dummies(features_transform)\n",
    "reference_columns = features_final.columns\n",
    "\n",
    "# Function to preprocess user input\n",
    "def preprocess_input(user_input):\n",
    "    # Convert user input to DataFrame\n",
    "    input_df = pd.DataFrame([user_input], columns=feature_columns)\n",
    "    \n",
    "    # Apply the same preprocessing steps as in the training script\n",
    "    input_df[['age', 'result']] = minmax_scaler.transform(input_df[['age', 'result']])\n",
    "    input_df[['age', 'result']] = standard_scaler.transform(input_df[['age', 'result']])\n",
    "    \n",
    "    input_df = pd.get_dummies(input_df)\n",
    "    \n",
    "    # Ensure all columns are present\n",
    "    for col in reference_columns:\n",
    "        if col not in input_df.columns:\n",
    "            input_df[col] = 0\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    input_df = input_df[reference_columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "# Function to get user input and make prediction\n",
    "def get_user_input_and_predict():\n",
    "    # Get user input\n",
    "    user_input = {\n",
    "        'age': float(input(\"Enter age (e.g., 12-16): \")),\n",
    "        'gender': input(f\"Enter gender ({'/'.join(unique_genders)}): \").strip().upper(),\n",
    "        'ethnicity': input(f\"Enter ethnicity ({'/'.join(unique_ethnicities)}): \").strip(),\n",
    "        'jundice': input(f\"Had jaundice ({'/'.join(unique_jundice)}): \").strip().lower(),\n",
    "        'autism': input(f\"Family member with autism ({'/'.join(unique_austim)}): \").strip().lower(),\n",
    "        'contry_of_res': input(f\"Enter country of residence ({'/'.join(unique_countries)}): \").strip(),\n",
    "        'result': float(input(\"Enter result (e.g., 10.0): \")),\n",
    "        'relation': input(f\"Enter relation ({'/'.join(unique_relations)}): \").strip(),\n",
    "        'A1_Score': int(input(\"Enter A1_Score (0 or 1): \")),\n",
    "        'A2_Score': int(input(\"Enter A2_Score (0 or 1): \")),\n",
    "        'A3_Score': int(input(\"Enter A3_Score (0 or 1): \")),\n",
    "        'A4_Score': int(input(\"Enter A4_Score (0 or 1): \")),\n",
    "        'A5_Score': int(input(\"Enter A5_Score (0 or 1): \")),\n",
    "        'A6_Score': int(input(\"Enter A6_Score (0 or 1): \")),\n",
    "        'A7_Score': int(input(\"Enter A7_Score (0 or 1): \")),\n",
    "        'A8_Score': int(input(\"Enter A8_Score (0 or 1): \")),\n",
    "        'A9_Score': int(input(\"Enter A9_Score (0 or 1): \")),\n",
    "        'A10_Score': int(input(\"Enter A10_Score (0 or 1): \"))\n",
    "    }\n",
    "    \n",
    "    # Preprocess the input\n",
    "    input_df = preprocess_input(user_input)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(input_df)\n",
    "    \n",
    "    # Print the prediction\n",
    "    if prediction[0] == 1:\n",
    "        print(\"The model predicts that the individual has ASD.\")\n",
    "    else:\n",
    "        print(\"The model predicts that the individual does not have ASD.\")\n",
    "\n",
    "# Call the function to get user input and make prediction\n",
    "get_user_input_and_predict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
